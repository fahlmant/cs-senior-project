\documentclass{article}
\newcommand{\namesigdate}[2][5cm]{%
  \begin{tabular}{@{}p{#1}@{}}
    #2 \\[2\normalbaselineskip] \hrule \\[0pt]
    {\small \textit{Signature}} \\[2\normalbaselineskip] \hrule \\[0pt]
    {\small \textit{Date}}
  \end{tabular}
}
\begin{document}
\title{Lidar Point Clouds and VR}
\author{Joshua Bowen, Taylor Fahlman, Adam Puckette}

\maketitle

\abstract

The goal is to create a virtual reality (VR) application that would allow users of most head-mounted virtual reality devices to view and manipulate point-cloud data. The solution will draw upon two existing codebases. First, the open source virtual reality platform (OSVR), which is a codebase designed with compatibility in mind. Second, the open source point-cloud visualization software CloudCompare, which is well-supported and handles multiple file formats. These two codebases will allow us to create modular and platform-independent software. This way, anyone with a virtual reality headset will be able to download and view point-cloud data without needing specialized hardware. In the solution, there will be a graphical user interface (GUI) which will allow the user to view and manipulate the point-cloud data from a first-person and third-person perspective. 

\newpage
\thispagestyle{empty}
\mbox{}
\section*{Problem Statement}

3D point clouds generated by lidar scanners are used to take precise measurements of physical structures. They do this by bouncing a laser off of their surroundings millions of times and recording the location of each bounce as a data point (hence "point cloud", a cloud of data points). This is often used to look for abnormalities in the environment that would be difficult to spot or measure otherwise. For example, a scan of an earthquake-damaged home can be analyzed by an architect to spot things like sagging walls and skewed foundations, things which would be difficult and dangerous to measure in person. However, there is a distinct lack of truly 3D point cloud viewing software. Point clouds are a 3D format, but most viewing software for them have been limited to 2D screens. This makes it difficult to manipulate the data quickly and efficiently. 3D televisions have been used as a solution to this problem, but they are expensive, complex, and not very portable. With our software, any analyst with a VR headset could view and manipulate this data in true stereoscopic 3D. This would allow them to do their jobs faster and more effectively, and keep hardware costs down. 

\section*{Proposed Solution}

The software we write will allow anyone with a VR headset to view and manipulate point cloud data. This is currently not possible, as the software has yet to be written. Our solution will draw upon both CloudCompare and the OSVR framework. By doing this, users of CloudCompare can then use whatever VR headset they have available to analyze and interact with their data in a 3D virtual reality space. This will allow for user-friendly, real-time selection and manipulation of 3D areas within point cloud files. In simpler terms, rather than sorting through a complex 2D interface, users will simply be able to reach out and take control of the data into their own hands. At the expo we plan to demo the software on laptop with an attached headset. People will be able to put it on and explore a point cloud, possibly one modeling the interior of the Kelly Engineering Center. 

\section*{Metrics for Success}

In order to measure performance and gauge success we will set several goals to be accomplished. Each goal will have a tier associated with it, to allow for different degrees of success. Our two goals will be framerate and cloud size. For framerate (measured in frames-per-second, or FPS), the tiers go as follows: 30 FPS, 60 FPS, and 90 FPS. For cloud size, we will begin by rendering a 1 million point cloud and work our way through 1 billion up to several billion, incrementing by nearly a billion points each tier. Therefore, the bare minimum for this project to be considered a success would be to render a 1 million point cloud at a constant 30 FPS. Time permitting, we also plan to use Leap Motion's gesture recognition capability to seamlessly respond to user commands.

\vspace{2pc}

\noindent \namesigdate{Matt O'Banion} \hfill \namesigdate[3cm]{Adam Puckette}

\vspace{2pc}

\noindent \namesigdate{Taylor Fahlman} \hfill \namesigdate[3cm]{Joshua Bowen}

\end{document}

